\chapter{Introducci√≥n}
\section{Vector Spaces}
\begin{definition}
    A vector space eover the numeric field $K$ ($K = \mathbb{R}$ or $K = \mathbb{C}$) is a nonempty set $V$, whose elements are called vector and in which two operations are defined, called addition and scalar multiplication, that enjoy the following properties:
    \begin{enumerate}
        \item addition is commutative and associative;
        \item there exists an element $0 \in V$ (the zero vector or null vector) such that $v + 0 = v$ for each $v \in V$
        \item $0 \cdot v = 0$, $1 \cdot v = v$, for each $v \in V$, where 0 and 1 are respectively the zero and the unity of $K$;
        \item for each element $v \in V$ there exists its opposite, $-v$, in $V$ such that $v + (-v) = 0$;
        \item the following distributive properties hold
        \[\forall \alpha \in K, \forall v,w \in V, \alpha(v + w) = \alpha v + \alpha w\]
        \[\forall \alpha , \beta \in K, \forall v \in V, (\alpha + \beta)v = \alpha v + \beta v\]
        \item the following associative property holds
        \[\forall \alpha , \beta \in K, \forall v \in V, (\alpha \beta)v = \alpha (\beta v)\]
    \end{enumerate}
\end{definition}

\begin{definition}
    We say that a nonempty part $W$ of $V$ is a vector subspace of $V$ if $W$ is a vector space over $K$.
\end{definition}

\begin{definition}
    A system of vector ${v_1, ..., v_n}$ of a vector space $V$ is called linearly independent if the relation
    \[\alpha_1 v_1 + \alpha_2 v_2 +...+ \alpha_m v_m = 0\]
    with $\alpha_1, \alpha_2, ..., \alpha_m \in K$ implies that $\alpha_1 = \alpha_2 = ...= \alpha_m = 0$. Otherwise, the system will be called linearly dependent.
\end{definition}

We call a basis of $V$ any system of linearly independent generator of $V$. If ${u_1, ..., u_n}$ is a basis of $V$, the expression $v = v_1 u_1 + ... + v_n u_n$ is called the decomposition of $v$ with respect to the basis and the scalars $v_1, ..., v_n \in K$ are the components of $v$ with respect to the given basis. Moreover, the following pro holds.

\begin{property}
    Let $V$ be a vector space which admits a basis of $n$ vectors. Then every system of linearly independent vector of $V$ hast at most $n$ elements and any other basis of $V$ has $n$ elements. The number $n$ is called the dimension of $V$ and we write $dim(V) = n$.
    If, instead, for any $n$ there always exist $n$ linearly independent vectors of $V$, the vector space is called infinite dimensional.
\end{property}

\section{Matrices}
Let $m$ and $n$ be two positive integers. We call a matrix having $m$ rows and $n$ columns, or a matrix $m \times n$, or a matrix $(m, n)$, with elements in $K$, a set of $mn$ scalars $a_{ij} \in K$, with $i = 1,...,m$ and $j = 1,...,n$, represented in the following rectangular array
\begin{equation}
    A = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
        \end{bmatrix}
    \label{matrices}
\end{equation}
When $K = \mathbb{R}$ or $K = \mathbb{C}$ we shall respectively write $A \in \mathbb{R}^{m \times n}$ or $A \in \mathbb{C}^{m \times n}$, to explicitly outline the numerical fields which the elements of $A$ belong to. Capital letters will be used to denote the matrices, while the lower case letters corresponding to those upper case letters will denote the matrix entries.

We shall abbreviate (\ref{matrices}) as $A = (a_ij)$ with $i = 1,...,m$ and $j = 1,...,n$. The index $i$ is called row index, while $j$ is the column index. The set $(a_{i1}, a_{i2}, ..., a_{in})$ is called the i-th row of $A$; likewise, $(a_{1j}, a_{2j}, ..., a_{mj})$ is the j-th column of $A$.

If $n = m$ the matrix is called squared or having order $n$ and the set of the entries $(a_{11}, a_{22}, ..., a_{nn})$ is called its main diagonal.

A matrix having one row or one column is called a row vector or column vector respectively. Unless otherwise specified, we shall always assume that a vector is a column vector. In the case $n = m = 1$, the matrix will simply denote a scalar of $K$.

\begin{definition}
    Let $A$ be a matrix $m  \times n$. Let $1 \leq i_1 < i_2 < ... < i_k \leq m$ and $1 \leq j_1 < j_2 < ... < j_l \leq n$ two sets of contiguous indexes. The matrix $S(k \times l)$ of entries $s_{pq} = a_{i_p j_q}$ with $p = 1, ..., k$, $q = 1, ..., l$ is called a submatrix of $A$. If $k = l$ and $i_r = j_r$ for $r = 1, ..., k$, $S$ is called a principal submatrix of $A$.
\end{definition}

\begin{definition}
    A matrix $A(m \times n)$ is called block partitioned or said to be partitioned into submatrices if 
    \[A = \begin{bmatrix}
        A_{11} & A_{12} & \cdots & A_{1l} \\
        A_{21} & A_{22} & \cdots & A_{2l} \\
        \vdots & \vdots & \ddots & \vdots \\
        A_{k1} & A_{k2} & \cdots & A_{kl}
    \end{bmatrix}\]
    where $A_{ij}$ are submatrices of $A$.
\end{definition}

\section{Operations with Matrices}
\subsection{Matrices and Linear Mappings}
\begin{definition}
    A linear map for $\mathbb{C}^n$ into $\mathbb{C}^m$ is a function $f: \mathbb{C}^n \rightarrow \mathbb{C}^m$ such that $f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$, $\forall \alpha , \beta \in K$ and $\forall x, y \in \mathbb{C}^n$.
\end{definition}

The following result links matrices and linear maps.

\begin{property}
    Let $f: \mathbb{C}^n \rightarrow \mathbb{C}^m$ be a linear map. Then, ther exists a unique matrix $A_f \in \mathbb{C}^{m \times n}$ such that
    \begin{equation}
        f(x) = A_f x \quad \forall x \in \mathbb{C}^n
        \label{Matrices and Linear Mappings}
    \end{equation}
    Conversely, if $A_f \in \mathbb{C}^{m \times n}$ then the function defined in (\ref{Matrices and Linear Mappings}) is a linear map from $\mathbb{C}^n$ into $\mathbb{C}^m$.
\end{property}

\section{Well-posedness and Condition Number of a Problem}
Consider the following problem: find $x$ such that
\begin{equation}
    F(x, d) = 0
    \label{problem}
\end{equation}
where $d$ is the set of data which the solution depends on and $F$ is the functional relation between $x$ and $d$. According to the kind of problem that is represented in (\ref{problem}), the variables $x$ and $d$ may be real numbers, vectors or functions. Typically, (\ref{problem}) is called a direct problem if $F$ and $d$ are given and $x$ is unknown, inverse problem if $F$ and $x$n are known and $d$ is the unknown, identigication problem when $x$ and $d$ are given while the functional relation $F$ is the unknown.

Problem (\ref{problem}) is well posed if it admits a unique solution $x$ which depends with continuaity on the data. We shall use the terms well posed and stable in an interchanging manner and we sall deal hecenforth only with well-posed problems.

A problem which does not enjoy the property above is called ill posed or unstable and before undertaking its numerical solution it has to be regularized, that is, it must be suitably transformed into a well-posed problem. Indeed, it is not appropriate to pretend the numerical method can cure the pathologies of an instrinsically ill-posed problem.

Let $D$ be the set of admissible data, i.e. the set of the values of $d$ in correspondance of which problem (\ref{problem}) admits a unique solution. Continuous dependence on the data means that small perturbations on the data $d$ of $D$ yield ``small'' changes in the solution $x$. Precisely, led $d \in D$ and denote by $\delta d$ a perturbation admissible in the sense that $d + \delta d \in D$ and by $\delta x$ the corresponding change in the solution, in such a way that
\begin{equation}
    F(x + \delta x, d + \delta d) = 0
    \label{equation 2.2}
\end{equation}
Then, we require that
\begin{multline}
    \exists \eta_0 =\eta_0(d) > 0, \quad \exists K_0 = K_0 (d) \text{ such that if } \\
     \| \delta d \| \leq \eta_0 \text{ then } \| \delta x \| \leq K_0 \| \delta d \|
     \label{equation(2.3)}
\end{multline}

The norms used fot the data and for the solution may not coincide, whenever $d$ and $x$ represent variables of different kinds.

\begin{remark}
    The property of continuous dependence on the data could have been stated in the following alternative way, which is more akin to the classical form of Analysis $\forall \epsilon > 0\ \exists \delta = \delta (\epsilon)$ such that if $\| \delta d \| \leq \delta$ then $\| \delta x \| \leq \epsilon$.
    
    The form (\ref{equation(2.3)}) is however more suitable to express in the following the concept of numerical stability, that is, the property that small perturbations on the data yield perturbations of the same order on the solution.
\end{remark}

With the aim of making the stability analysis more quantitative, we introduce the following definition.

\begin{definition}
    For problem (\ref{problem}) we define the relative condition number to be
    \begin{equation}
        K(d) = sup\{ \frac{\| \delta x \| / \| x \|}{\| \delta d \| / \|d \|}, \delta d \neq 0, d + \delta d \in D \}
        \label{equation 2.4}
    \end{equation}
    Whenever $d = 0$ or $x = 0$, it is necessary to introduce the absolute condition number, given by
    \begin{equation}
        K_{abs}(d) = sup \{ \frac{ \| \delta x \|}{ \| \delta d \|}, \delta d \neq 0, d + \delta d \in D \}
        \label{equation 2.5}
    \end{equation}
\end{definition}

Problem (\ref{problem}) is called ill-conditioned if  $K(d)$ is ``big'' for any admissible datum $d$.

The property of a problem of being well-conditioned is independent of the numerical method that is being used to solve it. In fact, it is possible to generate stable as well as unstable numerical schemes for solving well-conditioned problems. The concept of stability  for an algorithm or for a numerical method is analogous to that used for problem (\ref{problem}) and will be made precise in the next section.

\begin{remark}
    (Ill-posed problems) Even in the case in which the condition number does not exist (formally, it is infinite), it is not necessarily true that the problem is ill-posed. In fact there exist well posed problems for which the condition number is infinite, but suuch that they can be reformulated in equivalente problems with a finite condition number.
\end{remark}

If problem (\ref{problem}) admits a unique solution, then there necessarily exists a mappin $G$, that we call resolvent, between the sets of the data and of the solutions, such that
\begin{equation}
    x = G(d) \text{, that is } F(G(d), d) = 0
    \label{equation 2.6}
\end{equation}
According to this definition, (\ref{equation 2.2}) yields $x + \delta x = G(d + \delta d)$. Assuming that $G$ is differentiable in $d$ and denoting formally by $G'(d)$ its derivative with respect to $d$ (if $G: \mathbb{R}^n \rightarrow \mathbb{R}^m, G'(d)$ will be the Jacobian matrix of $G$ evaluated at the vector $d$), a Taylor's expansion of $G$ truncated at first order ensures that 
\[G(d + \delta d) - G(d) n= G'(d) \delta d + o(\| \delta d \|) \quad \text{for } \delta d \rightarrow 0 \]
wheere $\| \cdot \|$ is a suitable vector norm and $o(\cdot)$ is the classical infinitesimal symbol denoting an infinitesimal term of higher order with respect to its argument. Neglecting the infinitesimal of higher order with respect to $\| \|delta d \|$, from (\ref{equation 2.4}) and (\ref{equation 2.5}) we respectively deduce that
\begin{equation}
    K(d) \approx \| G'(d) \| \frac{\| d \|}{\| G(d) \|}, \quad K_{abs}(d) \approx \| G'(d) \|
    \label{equation 2.7}
\end{equation}
where the symbol $\| \cdot \|$, when applied to a matrix, denotes the induced matrix norm (\ref{equation 1.19}) associated with the vector norm introduced above. The estimates in (\ref{equation 2.7}) are of great practical usefulness in the analysis of problems in the form (\ref{equation 2.6}).

\begin{theorem}
    Let $\| \cdot \|$ be a vector norm. The function
    \begin{equation}
        \| A \| = \sup_{x \neq 0} \frac{\| A x \|}{\|x \|}
        \label{equation 1.19}
    \end{equation}
    is a matrix norm called induced matrix norm or natural matrix norm.
\end{theorem}

In view of (\ref{equation 2.7}), the quantity $\| G'(d) \|$ is an approximation of $K_{abs}(d)$ and is sometimes called first order absolute condition number. This latter represents the limit of the Lipschitz constant of $G$ as the perturbation on the data tends to zero.

Such a number does not always provide a sound estimate of the condition number $K_{abs}(d)$. This happends, for instance, when $G'$ vanishes at a point whilst $G$ is nonnull in a neightborhood of the same point.