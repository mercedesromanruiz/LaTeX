\chapter{Número de Condición de una Matriz}
\section{Matrix Norms}
\begin{definition}
    A matrix norm is a mapping $\|\cdot\|: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$ such that:
    \begin{enumerate}
        \item $\|A\| \geq 0 \forall A \in \mathbb{R}^{m \times n}$ and $\|A\| = 0$ if and only if $A = 0$;
        \item $\|\alpha A\| = |\alpha| \|A\| \forall \alpha \in \mathbb{R}, \forall A \in \mathbb{R}^{m \times n}$ (homogeneity);
        \item $\| A + B\| \leq \|A\| + \|B\| \forall A, B \in \mathbb{R}^{m \times n}$ (triangular inequality).
    \end{enumerate}
    \label{Def: 1.19}
\end{definition}

Unless otherwise specified we shall employ the same symbol $\|\cdot\|$, to denote matrix norms and vector norms.

We can better characterize the matriz norms by introducing the concepts of compatible norm and norm induced by a vector norm.

\begin{definition}
    We say that a matrix norm $\|\cdot\|$ is compatible or consistent with a vector norm $\|\cdot\|$ if 
    \begin{equation}
        \|Ax\| \leq \|A\| \|x\|, \quad \forall x \in \mathbb{R}^n
        \label{Eq: (1.16)}
    \end{equation}
    More generally, given three norms, all denoted by $\|\cdot\|$, albeit defined on $\mathbb{R}^m$, $\mathbb{R}^n$ and $\mathbb{R}^{m \times n}$, respectively, we say that they are consistent if $\forall x \in \mathbb{R}^n$, $Ax = y \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, we hace that $\|y\| \leq \|A\| \|x\|$
\end{definition}

In order to single out matrix norms of practical interest, the followinf property is in general required

\begin{definition}
    We say that a matriz norm $\| \cdot \|$ is sub-multiplicative if $\forall A \in \mathbb{R}^{m \times n}$, $\forall B \in \mathbb{R}^{m \times q}$
    \begin{equation}
        \| A B\| \leq \|A\| \|B\|
        \label{Eq: (1.17)}
    \end{equation}
\end{definition}

This property is not satisfied by any matrix norm. For example, the norm $ \|A\|_\Delta = \max{|a_ij|}$ for $i = 1, ..., n$, $j = 1, ..., m$ does nor satisfy (\ref{Eq: (1.17)}) if applied to the matrices
\[ A = B = \begin{bmatrix}
    1 & 1 \\
    1 & 1
\end{bmatrix} \]
since $2 = \|AB\|_\Delta < \|A\|_\Delta \|B\|_\Delta = 1$.

Notice that, given a certain sub-multiplicative matrix norm $\|\cdot\|_\alpha$, there always exists a consistent vector norm. For instance, given any fixed vector $y \neq 0$ in $\mathbb{C}^n$, it suffices to define the consistent vector norm as
\[ \|x\| = \| x y^H \|_\alpha \quad x \in \mathbb{C}^n\]

As a consequence, in the case of sub-multiplicative matrix norms it is no longer necessary to explicitly specify the vector norm with respect to the matrix norm is consistent.

In view of the definition of a natural norm, we recall the following theorem.

\begin{theorem}
    Let $\| \cdot \|$ be a vector norm. The function
    \begin{equation}
        \| A \| = \sup_{x \neq 0}{\frac{\| Ax \|}{\|x\|}}
        \label{Eq: (1.19)}
    \end{equation}
    is a matrix norm called induced matrix norm or natural matrix norm.
    \label{Theorem: 1.1}
\end{theorem}

\begin{proof}
    We start by noticing that (\ref{Eq: (1.19)}) is equivalent to 
    \begin{equation}
        \|A\| = \sup_{\|x\| = 1}{\|Ax\|}
        \label{Eq: (1.20)}
    \end{equation}
    Indeed, one can define for any $x \neq 0 $ the unit vector $u = x / \|x\|$, so that (\ref{Eq: (1.19)}) becomes
    \[ \|A\| = \sup_{\|u\| = 1}{\|Au\|} = \|Aw\| \quad \|w\| = 1 \]
    This being taken as given, let us check that (\ref{Eq: (1.19)}) (or, equivalently, (\ref{Eq: (1.20)})) is actually a norm, making direct use of Definition \ref{Def: 1.19}
    \begin{enumerate}
        \item If $\|Ax\| \geq 0$, then it follows that $\|A\| =  \sup_{\|x\| = 1}{\|Ax\| \geq 0}$. Moreover
        \[ \|A\| = \sup_{\|x \neq 0}{\frac{\|Ax\|}{\|x\|}} = 0 \Leftrightarrow \|Ax\| = 0 \forall x \neq 0\]
        and $Ax = 0 \forall x \neq 0$ if and only if $A = 0$; therefore $\|A\| = 0 \Leftrightarrow A = 0$.
        \item Given a scalar $\alpha$
        \[ \|\alpha A\| = \sup_{\|x\|=1}{\|\alpha A x\|} = |\alpha| \sup_{\|x\| = 1} {\|Ax\|} = |\alpha| \|A\| \]
        \item Finally, triangular inequality holds. Indeed, by definition of suupremum, if $x \neq 0$ then 
        \[ \frac{\|Ax\|}{\|x\|} \leq \|A\| \Rightarrow \|Ax\| \leq \|A\| \|x\|\]
        so that, taking $x$ with unit norm, one gets 
        \[ \|(A + B)x\| = \leq \|Ax\| + \|Bx\| \leq  \|A\| + \|B\| \]
        from which it follows that $\|A+B\| = \sup_{\|x\|= 1}{\|(A+ B) x\|} \leq \|A\| + \|B\|$
    \end{enumerate}
\end{proof}

Relevant instances of induces matrix norms are the so-called p-norms defined as 
\[\|A\|_p = \sup_{x \neq 0}{\frac{\|Ax\||_p}{\|x\|_p}}\]

The 1-nrom and the infinity norm are easily computable since
\[ \|A\|_1 = \max_{j = 1,...,n}{\sum_{i = 1}^{m}{|a_{ij}|}}, \quad \|A\|_\infty = \max_{i=1,...,m}{\sum_{j = 1}^{n}{|a_{ij}|}} \]
and they are called the column sum norm and the row sum norm, respectively.

Moreover, we have $\|A\|_1 = \|A^T\|_\infty$ and, if A is self-adjoint or real symetric, $\|A\|_1 = \|A\|_\infty$.

A special discussion is deserved by the 2-norm or spectral norm for which the following theorem holds.

\begin{theorem}
    Let $\sigma_1(A)$ be the largest sigular value of A. Then
    \begin{equation}
        \|A\|_2 = \sqrt{\rho (A^H A)} = \sqrt{\rho (A A^H)} = \sigma_1 (A)
        \label{Eq: (1.21)}
    \end{equation}
    In particular, if A is hermitian (or real symmetric), then
    \begin{equation}
        \|A\|_2 = \rho(A)
        \label{Eq: (1.22)}
    \end{equation}
    while, if $A$ is unitary, $\|A\|_2 = 1$.
\end{theorem}

\begin{proof}
    Since $A^H A$ is hermitian, there exists an unitary matrix $U$ such that
    \[ U^H A^H A U = diag(\mu_1,...,\mu_n) \]
    where $\mu_i$, are the (positive) eigenvalues of $A^H A$. Let $y = U^H x$, then
    \[ \|A\|_2 = \sup_{x \neq 0}\sqrt{\frac{(A^H A x, x)}{(x, x)}} = \sup_{y \neq 0}\sqrt{\frac{(U^H A^H A Uy, y)}{(y,y)}} \]
    \[ = \sup_{y \neq 0}\sqrt{\sum_{i = 1}^{n} \mu_i |y_i|^2 / \sum_{i = 1}^{n}}|y_i|^2 = \sqrt{\max_{i = 1,...,n} |\mu_i| } \]
    from which (\ref{Eq: (1.21)}) follows, thanks to % --------------

    If $A$ is hermitian, the same considerations as above apply directly to $A$. Finally if $A$ is unitary, we have
    \[ \|Ax\|_2^2 = (Ax, Ax) = (x, A^H Ax) = \|x\|_2^2 \]
    so that $\|A\|_2 = 1$.
\end{proof}

As a consequence, the computation of $\|A\|_2$ is much more extensive thant that of $\|A\|_\infty$ or $\|A\|_1$. However, if only an estimate of $\|A\|_2$ is required, the following relations can be profitably employed in the case of square matrices
\[ \max_{i,j}{|a_{ij}|} \leq \|A\|_2 \leq n \max_{i,j}{a_{ij}} \]
\[ \frac{1}{\sqrt{n}} \|A\|_\infty \leq \|A\|_2 \leq \sqrt{n} \|A\|_\infty \]
\[ \frac{1}{\sqrt{n}} \|A\|_1 \leq \|A\|_2 \leq \sqrt{n} \|A\|_1 \]
\[ \|A\|_2 \leq \sqrt{\|A\|_1 \|A\|_\infty} \]
Moreover, if $A$ is normal then $\|A\|_2 \leq \|A\|_p$ for any $n$ and all $ p \geq 2 $.

\begin{theorem}
    Let $ \||\cdot|\| $ be a matrix norm induced by a vector norm $\| \cdot \|$. Then, the following relations hold:
    \begin{enumerate}
        \item $ \|Ax\| \leq \||A|\|  \|x\| $, that is $ \||\cdot|\| $ is a norm compatible with $\|\cdot\|$;
        \item $\||I|\| = 1 $
        \item $\||AB|\| \leq \||A|\| \||B|\|$, that is, $ \||\cdot|\| $ is sub-multiplicative.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Part 1 of the theorem is already contained in the proof of Theorem \ref{Theorem: 1.1}, while part 2 follows from the fact that $ \||I|\| = \sup_{x \neq 0}{\|Ix\| / \|x\|} = 1 $. Part 3 is simple to check. 
\end{proof}

Notice that the p-norms are sub-multiplicative. Moreover, we remark that the sub-multiplicativity property by itself would only allow us to conclude that $\||I|\| \geq 1$. Indeed, $\||I|\| = \||I \cdot I|\| \leq \||I|\|^2$.

\section{Stability Analysis of Linear Systems}

\subsection{The Condition Number of a Matrix}

The condition number of a matrix $A \in \mathbb{C}^{m \times n}$ is defined as
\begin{equation}
    K(A) = \|A\| \|A^{-1}\|
    \label{Eq: (3.4)}
\end{equation}
where $\|\cdot\|$ is an induced matrix norm. 